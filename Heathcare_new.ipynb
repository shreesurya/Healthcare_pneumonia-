{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bare Bones \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import shutil\n",
    "import urllib.request\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Tensorflow and Keras \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization, \\\n",
    "  GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, \\\n",
    "  preprocess_input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, \\\n",
    "  ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Some utils \n",
    "from itertools import product\n",
    "from functools import partial\n",
    "\n",
    "# Mount the Drive if in Google Colaboratory\n",
    "try:\n",
    "    from google.colab import drive\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since I am running this notebook on Colab, \n",
    "# Let's try and get some system information \n",
    "import platform\n",
    "print('System Processor: ', platform.processor(), '\\n')\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount the Google Drive \n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/content/drive/My Drive/siraj_week4/'\n",
    "\n",
    "train_dir =  filepath + 'train/'\n",
    "validation_dir = filepath + 'val/'\n",
    "test_dir = filepath + 'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of classes we are trying to classify are 2\n",
    "# Normal or Pneumonia\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Build the Model\n",
    "def create_model(input_shape, num_classes):\n",
    "  \"\"\"\n",
    "  Creates the image classifier model on top of a\n",
    "  pretrained Inception v3.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  input_shape : Tuple    \n",
    "      Shape tuple for Inception v3\n",
    "  \n",
    "  num_classes : Integer\n",
    "      The number of classes that the classifier is \n",
    "      going to classify\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  The tensorflow model.  \n",
    "  \"\"\"\n",
    "  # We start with a blank slate\n",
    "  K.clear_session()\n",
    "\n",
    "  # Inception v3 is our base model\n",
    "  base_model = InceptionV3(weights='imagenet', include_top=False,\\\n",
    "                           input_shape=input_shape)\n",
    "  \n",
    "  # Add our custom layers on top for classification\n",
    "  x = base_model.output\n",
    "  x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "  x = Dense(512, activation='relu')(x)\n",
    "  x = Dropout(0.3)(x)\n",
    "  x = Dense(256, activation='relu')(x)\n",
    "  x = Dropout(0.3)(x)\n",
    "  x = Dense(128, activation='relu')(x)\n",
    "  x = Dropout(0.3)(x)\n",
    "  \n",
    "  # We are using the base model only for feature extraction\n",
    "  # So we make sure the layer weights don't change \n",
    "  for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "  # The final predictions layer is going to be a dense layer\n",
    "  # with Sigmoid activations - predicts only 'num_classes'    \n",
    "  predictions = Dense(num_classes, activation='sigmoid')(x)\n",
    "  # Instantiate the model \n",
    "  model = Model(inputs=base_model.inputs, outputs=predictions)\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "# Creat the model with Height & Width as 150 and 3 channels for Inception\n",
    "# And we are going to predict only 2 classes in our case\n",
    "model = create_model((150, 150, 3), NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss\n",
    "training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
    "# Training Accuracy\n",
    "training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    'training_accuracy', dtype=tf.float32)\n",
    "# Test Loss \n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "# Test Accuracy \n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    'test_accuracy', dtype=tf.float32)\n",
    "\n",
    "# Print model summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the standard Adam Optimizer \n",
    "optimizer = Adam(lr=0.0001)\n",
    "\n",
    "# Compile \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_file_count(directory):\n",
    "  \"\"\"\n",
    "  Counts the total number of files in a directory. \n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  directory : String    \n",
    "      The directory isnide which the function is going\n",
    "      to count the number of files \n",
    "  \n",
    "  Returns\n",
    "  -------\n",
    "  Total number of files present inside the 'directory'.  \n",
    "  \"\"\"\n",
    "  return sum([len(files) for r, d, files in os.walk(directory)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters \n",
    "rescale = 1./255\n",
    "target_size = (150, 150)\n",
    "batch_size = 500\n",
    "class_mode = 'categorical'\n",
    "\n",
    "# Augment the Training dataset images \n",
    "train_datagen = ImageDataGenerator(rescale=rescale,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.2)\n",
    "# Load the images in the generator \n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=target_size,\n",
    "                                                    class_mode=class_mode,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True)\n",
    "# Augment the validation dataset images\n",
    "val_datagen = ImageDataGenerator(rescale=rescale)\n",
    "# Load the images in the generator\n",
    "val_generator = val_datagen.flow_from_directory(validation_dir, \n",
    "                                                target_size=target_size,\n",
    "                                                class_mode=class_mode,\n",
    "                                                batch_size=dir_file_count(validation_dir),\n",
    "                                                shuffle=False)\n",
    "# Augment the test dataset images\n",
    "test_datagen = ImageDataGenerator(rescale=rescale)\n",
    "# Load the images in the generator\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=target_size,\n",
    "                                                  class_mode=class_mode,\n",
    "                                                  batch_size=dir_file_count(test_dir),\n",
    "                                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_generator.classes\n",
    "labels = np.unique(y)\n",
    "\n",
    "train_class_weights = compute_class_weight('balanced', labels, y)\n",
    "print(train_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=len(train_generator),\n",
    "                              epochs=10,\n",
    "                              verbose=1,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=len(val_generator),\n",
    "                              class_weight=train_class_weights,\n",
    "                              workers=20)\n",
    "\n",
    "# Save the model after the training is complete\n",
    "MODEL_FILE = 'pneumonia_v1.hd5'\n",
    "model.save(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the trained model to the Google Drive \n",
    "!mv {MODEL_FILE} '/content/drive/My Drive/siraj_week4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model from Google Drive \n",
    "MODEL_FILE = f'/content/drive/My Drive/siraj_week4/{MODEL_FILE}'\n",
    "model = tf.keras.models.load_model(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on Test Data \n",
    "result = model.evaluate_generator(test_generator, steps=len(test_generator),\\\n",
    "                                  verbose=1)\n",
    "\n",
    "print(\"%s%.2f  \"% (\"Loss     : \", result[0]))\n",
    "print(\"%s%.2f%s\"% (\"Accuracy : \", result[1]*100, \"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly generate the Test Batch No\n",
    "num_of_batch = len(test_generator) # This is 1 in our case \n",
    "batch_no = random.randint(0, num_of_batch - 1)\n",
    "\n",
    "# Fetch the batch data\n",
    "y_img_batch, y_true_batch = test_generator.__getitem__(batch_no)\n",
    "y_true_batch = y_true_batch.argmax(axis=-1)\n",
    "\n",
    "# Make the predictions \n",
    "y_pred_batch = model.predict(y_img_batch)\n",
    "y_pred_batch = y_pred_batch.argmax(axis=-1)\n",
    "\n",
    "# Print the results \n",
    "print(\"-\"*35)\n",
    "print(\"%s%d\"%     (\"Selected Batch No       : \", batch_no))\n",
    "print(\"-\"*35)\n",
    "print(\"%s%d\"%     (\"Batch Size              : \", len(y_pred_batch)))\n",
    "print(\"-\"*35)\n",
    "print(\"%s%.2f%s\"% (\"Accuracy                : \", np.mean(y_true_batch==y_pred_batch)*100, \"%\"))\n",
    "print(\"-\"*35)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
